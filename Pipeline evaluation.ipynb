{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from src.evaluation.evaluate import predicted_ranks\n",
    "from src.datasets import dataset_factory\n",
    "from src.evaluation.metrics import standard_metrics\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_paths = [\n",
    "    'sampled_posts-mistral-123b-k_50.csv',\n",
    "    'sampled_posts-c4ai-104b-k_50.csv',\n",
    "    'sampled_posts-llama3_1-70b-k_50.csv',\n",
    "    'sampled_posts-llama3_3-70b-k_50.csv',\n",
    "    'sampled_posts-qwen2_5-72b-k_50.csv',\n",
    "    'sampled_posts-qwen2_5-7b-k_50.csv',\n",
    "    'sampled_posts-llama3_1-8b-k_50.csv',\n",
    "    'sampled_posts-gemma3-27b-k_50.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_factory(\n",
    "    name=\"multiclaim\",\n",
    "    crosslingual=False,\n",
    "    language=None,\n",
    "    split=None,\n",
    "    version=\"original\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_metrics(post_ids, fact_check_ids):\n",
    "    desired_fact_check_ids = defaultdict(lambda: list())\n",
    "    for fact_check_id, post_id in dataset.fact_check_post_mapping:\n",
    "        desired_fact_check_ids[post_id].append(fact_check_id)\n",
    "\n",
    "    predicted_ids = [\n",
    "        desired\n",
    "        for desired in fact_check_ids\n",
    "    ]\n",
    "    \n",
    "    ranks = []\n",
    "    for predicted_ids, post_id in zip(predicted_ids, post_ids):\n",
    "        desired_ids = desired_fact_check_ids[post_id]\n",
    "        post_ranks = predicted_ranks(np.array(predicted_ids), np.array(desired_ids), default_rank=100)\n",
    "        ranks.append(post_ranks.values())\n",
    "\n",
    "    return standard_metrics(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv('./datasets/multiclaim/sampled_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_results_df = pd.DataFrame()\n",
    "veracity_results_df = pd.DataFrame()\n",
    "df_true = pd.read_csv('./datasets/multiclaim/sampled_posts.csv')\n",
    "df_mapping = pd.read_csv('./datasets/multiclaim/fact_check_post_mapping.csv')\n",
    "\n",
    "sample_df = pd.DataFrame(columns=['post_id', 'relevant_claims_ids', 'model', 'prediction', 'ground_truth', 'explanation_prompt', 'explanation'])\n",
    "\n",
    "veracity_mapping = {\n",
    "    'true': 'true',\n",
    "    'false': 'false',\n",
    "    'unverifiable': 'unverifiable',\n",
    "    'partly true': 'true',\n",
    "    'partly false': 'false',\n",
    "    '': 'unverifiable',\n",
    "    'partially true': 'true',\n",
    "    'mostly true': 'true',\n",
    "    'misleading': 'false',\n",
    "    'not categorized': 'unverifiable',\n",
    "    'partly_true': 'true',\n",
    "    'mixture': 'unverifiable',\n",
    "}\n",
    "\n",
    "for path in result_paths:\n",
    "    df = pd.read_csv(f'./results/final/pipeline/{path}')\n",
    "    df['fact_check_ids'] = df['fact_check_ids'].apply(eval)\n",
    "    columns = [\n",
    "        'fact_check_ids',\n",
    "        'summaries',\n",
    "        'relevant_claims_ids',\n",
    "    ]\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna('[]')\n",
    "        df[column] = df[column].apply(lambda x: eval(str(x)))\n",
    "        \n",
    "    \n",
    "    post_ids = [int(post_id) for post_id in list(df['post_id'])]\n",
    "    fact_check_ids = df['fact_check_ids'].tolist()\n",
    "    \n",
    "    # get all the pairs\n",
    "    matching_df = pd.DataFrame()\n",
    "    for index, row in df.iterrows():\n",
    "        fc_ids = row.fact_check_ids\n",
    "        post_id = row.post_id\n",
    "\n",
    "        for fc_id in fc_ids:\n",
    "            matching_df = pd.concat([\n",
    "                matching_df,\n",
    "                pd.DataFrame({\n",
    "                    'post_id': [post_id],\n",
    "                    'fc_id': [fc_id]\n",
    "                })\n",
    "            ])\n",
    "            \n",
    "    matching_df = matching_df.reset_index()\n",
    "    matching_df['true_relevant'] = ''\n",
    "    for index, row in matching_df.iterrows():\n",
    "        post_id = row.post_id\n",
    "        fc_id = row.fc_id\n",
    "\n",
    "        found = df_mapping[(df_mapping['fact_check_id'] == fc_id) & ((df_mapping['post_id'] == post_id))]\n",
    "        if found.shape[0] > 0:\n",
    "            matching_df.at[index, 'true_relevant'] = 'yes'\n",
    "        else:\n",
    "            matching_df.at[index, 'true_relevant'] = 'no'\n",
    "            \n",
    "    matching_df[path] = 'no'\n",
    "    \n",
    "    # Evaluate the performance of Multilingual E5 Large model for claim retrieval\n",
    "    embedding_results = calculate_standard_metrics(post_ids, fact_check_ids)\n",
    "\n",
    "    # from tuples select only the first element\n",
    "    for key in embedding_results:\n",
    "        embedding_results[key] = embedding_results[key][0]\n",
    "    embedding_results['model'] = 'Multilingual E5 Large'\n",
    "    embedding_results['macro_f1'] = 0\n",
    "    embedding_results['accuracy'] = 0\n",
    "    embedding_results['macro_precision'] = 0\n",
    "    embedding_results['macro_recall'] = 0\n",
    "    embedding_results['tnr'] = 0\n",
    "    embedding_results['fnr'] = 0\n",
    "    embedding_results['tpr'] = 0\n",
    "    embedding_results['fpr'] = 0\n",
    "    \n",
    "    embedding_results_df = pd.concat([embedding_results_df, pd.DataFrame(embedding_results, index=[0])])\n",
    "    \n",
    "    # Evaluate the performance of the particular model for claim retrieval\n",
    "    relevant_claims_ids = df['relevant_claims_ids'].tolist()\n",
    "    model_embedding_results = calculate_standard_metrics(post_ids, relevant_claims_ids)\n",
    "    for key in model_embedding_results:\n",
    "        model_embedding_results[key] = model_embedding_results[key][0]\n",
    "    model_embedding_results['model'] = path\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        relevant_claims_ids = row.relevant_claims_ids\n",
    "        post_id = row.post_id\n",
    "\n",
    "        for fc_id in relevant_claims_ids:\n",
    "            found = matching_df[(matching_df['fc_id'] == fc_id) & ((matching_df['post_id'] == post_id))]\n",
    "            if found.shape[0] > 0:\n",
    "                matching_df.at[found.index[0], path] = 'yes'\n",
    "                \n",
    "    # Retrieval as Classification\n",
    "    y_true = list(matching_df['true_relevant'].values)\n",
    "    y_predict=list(matching_df[path].values)\n",
    "    macro_f1_retrieval =  f1_score(y_true, y_predict, average=\"macro\")\n",
    "    accuracy_retrieval = accuracy_score(y_true, y_predict)\n",
    "    macro_precision_retrieval = precision_score(y_true, y_predict, average=\"macro\")\n",
    "    macro_recall_retrieval = recall_score(y_true, y_predict, average=\"macro\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_predict).ravel()\n",
    "    tnr = tn / (tn + fp) # irrelevant filtration\n",
    "    fnr = fn / (fn + tp) # relevant filtration\n",
    "    tpr = tp / (tp + fn) # relevant recall\n",
    "    fpr = fp / (fp + tn) # irrelevant recall\n",
    "    \n",
    "    model_embedding_results['macro_f1'] = macro_f1_retrieval\n",
    "    model_embedding_results['accuracy'] = accuracy_retrieval\n",
    "    model_embedding_results['macro_precision'] = macro_precision_retrieval\n",
    "    model_embedding_results['macro_recall'] = macro_recall_retrieval\n",
    "    model_embedding_results['tnr'] = tnr\n",
    "    model_embedding_results['fnr'] = fnr\n",
    "    model_embedding_results['tpr'] = tpr\n",
    "    model_embedding_results['fpr'] = fpr\n",
    "\n",
    "    embedding_results_df = pd.concat([embedding_results_df, pd.DataFrame(model_embedding_results, index=[1])])\n",
    "\n",
    "    # Evaluate the performance of model for veracity prediction\n",
    "    df['ground_truth'] = ''\n",
    "    for index, row in df.iterrows():\n",
    "        post_id = row['post_id']\n",
    "        label = df_true[df_true['post_id'] == post_id]['rating'].values[0]\n",
    "        df.at[index, 'ground_truth'] = label.lower()\n",
    "        \n",
    "    df['predicted_veracity'] = df['explanation'].apply(lambda x: re.findall(r'\"veracity\": \"(.*)\"', str(x)))\n",
    "    df['predicted_veracity'] = df['predicted_veracity'].apply(lambda x: x[0].lower() if len(x) > 0 else '')\n",
    "\n",
    "    print(path)\n",
    "    print(df.predicted_veracity.unique())\n",
    "\n",
    "    df['predicted_veracity'] = df['predicted_veracity'].map(veracity_mapping)\n",
    "    print(df.predicted_veracity.unique())\n",
    "    \n",
    "    # empty string is predicted_veracity are considere as unverifiable\n",
    "    # df['predicted_veracity'] = df['predicted_veracity'].apply(lambda x: 'unverifiable' if x == '' else x)\n",
    "\n",
    "        \n",
    "    y_true = list(df['ground_truth'].values)\n",
    "    y_pred = list(df['predicted_veracity'].values)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    macro_f1 = report['macro avg']['f1-score']\n",
    "    accuracy = report['accuracy']\n",
    "    macro_precision = report['macro avg']['precision']\n",
    "    macro_recall = report['macro avg']['recall']\n",
    "    \n",
    "    true_count = len(df[df['predicted_veracity'] == 'true'])\n",
    "    false_count = len(df[df['predicted_veracity'] == 'false'])\n",
    "    unverifiable_count = len(df[df['predicted_veracity'] == 'unverifiable'])\n",
    "    \n",
    "    veracity_results = {\n",
    "        'model': path,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'true_count': true_count,\n",
    "        'false_count': false_count,\n",
    "        'unverifiable_count': unverifiable_count,\n",
    "    }\n",
    "    veracity_results_df = pd.concat([veracity_results_df, pd.DataFrame(veracity_results, index=[0])])\n",
    "    \n",
    "    # randomly sample 20 incorrect predictions\n",
    "    incorrect_predictions = df[(df['ground_truth'] != df['predicted_veracity']) & (~df['explanation'].isna())]\n",
    "    incorrect_predictions = incorrect_predictions.sample(20)\n",
    "        \n",
    "    for i, row in incorrect_predictions.iterrows():\n",
    "        explanation_text = re.findall(r'\"explanation\": \"(.*)\"', str(row['explanation']))\n",
    "        if len(explanation_text) == 0:\n",
    "            explanation_text = ['']\n",
    "        \n",
    "        sample_df = pd.concat([sample_df, pd.DataFrame({\n",
    "            'post_id': [row['post_id']],\n",
    "            'relevant_claims_ids': [row['relevant_claims_ids']],\n",
    "            'model': [path],\n",
    "            'prediction': [row['predicted_veracity']],\n",
    "            'ground_truth': [row['ground_truth']],\n",
    "            'predicted_explanation': [row['explanation']],\n",
    "            'explanation_prompt': [row['explanation_prompt']],\n",
    "            'explanation': [explanation_text[0]]\n",
    "        })])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_results_df.drop_duplicates(subset=['model'], keep='first', inplace=True)\n",
    "embedding_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracity_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disai-multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
